{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlocking the Palate - Evaluating Taste Consensus Among Beer Reviewers\n",
    "\n",
    "---\n",
    "\n",
    "Group [**BlackAda**](https://en.wikipedia.org/wiki/Blackadder)\n",
    "\n",
    "> - Ludek Cizinsky ([ludek@cizinsky@epfl.ch](ludek.cizinsky@epfl.ch))\n",
    "> - Peter Nutter ([peter@nutter@epfl.ch](peter@nutter@epfl.ch))\n",
    "> - Pierre Lardet ([pierre@lardet@epfl.ch](pierre@lardet@epfl.ch))\n",
    "> - Christian Bastin ([christian@bastin@epfl.ch](christian@bastin@epfl.ch))\n",
    "> - Mika Senghaas ([mika@senghaas@epfl.ch](mika@senghaas@epfl.ch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "---\n",
    "\n",
    "Navigating the world of beer reviews can be a daunting task for non-experts. Beer aficionados often describe brews as having nuanced flavors such as \"grassy notes\" and \"biscuity/ crackery malt,\" with hints of \"hay.\" But do these descriptions reflect the actual tasting experience? Following a \"wisdom-of-the-crowd\" approach, a descriptor can be considered meaningful if many, independent reviewers use similar descriptors for a beer's taste. To quantify consensus, we use natural language processing techniques to extract descriptors of a beer's taste and numerically represent these descriptors to compute similarity or consensus scores. The consensus scores between beer reviews will unveil whether there is a shared understanding of taste among beer geeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "---\n",
    "\n",
    "We load the dependencies required for this project to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable continuous module reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport src\n",
    "\n",
    "# Standard library\n",
    "import os\n",
    "\n",
    "# Custom modules\n",
    "from src import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And set some global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for the full dataset\n",
    "DATA_URL = \"https://drive.google.com/u/2/uc?id=1IqcAJtYrDB1j40rBY5M-PGp6KNX-E3xq&export=download\"\n",
    "\n",
    "# Number of samples to use for the subset\n",
    "NUM_SUBSET_SAMPLES = 10000\n",
    "\n",
    "# Paths\n",
    "ROOT_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n",
    "\n",
    "# Ensure data directory exists\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "---\n",
    "\n",
    "We will be working with the beer review data from the [BeerAdvocate](https://www.beeradvocate.com/) platform. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Download\n",
    "\n",
    "Due to its size (uncompressed 1.6 GB), the dataset is not included in the repository but must be downloaded. The course staff has provided the data via Google Drive. On the first run of this notebook, we download the compressed data file from Google Drive and extract it to the `data` folder. The compressed file is ~1.5 GB in size. \n",
    "\n",
    "After extraction and removing of unnecessary files (archives, ratings file, ...), the data folder should contain the following files: `beers.csv`, `breweries.csv`, `users.csv`, `reviews.txt`. The total size of the data is ~2.9 GB.\n",
    "\n",
    "*NB: Data loading takes around **~8min** on the first run. Subsequent runs of this cell are instant.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the BeerAdvocate dataset if it doesn't exist\n",
    "if not os.path.exists(os.path.join(DATA_DIR, \"reviews.txt\")):\n",
    "    utils.download_data(DATA_URL, data_dir=DATA_DIR)\n",
    "print(f\"Beer reviews downloaded to {DATA_DIR} ✅.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "Next, we load the data into a Pandas DataFrame. On the first run, we load all the reviews from the `reviews.txt` file and populate it with some additional meta-data from the other files. We then save the DataFrame to a `.feather` file for faster loading in the future. On subsequent runs, we load the DataFrame from the `.feather` file if it exists.\n",
    "\n",
    "*NB: Running this cell for the first time reads in all `2.5M` reviews which takes **~7min**. Subsequent runs should be much faster, taking about **~1min**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all reviews and a subset of reviews (10,000)\n",
    "reviews = utils.load_data(DATA_DIR)\n",
    "sub_reviews = utils.load_data(DATA_DIR, num_samples=NUM_SUBSET_SAMPLES)\n",
    "\n",
    "print(f\"Loaded {len(reviews)} reviews ✅. (+{len(sub_reviews)} reviews in subset)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA\n",
    "\n",
    "Let's explore the data a bit. We will analyse:\n",
    "\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "std",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
