{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlocking the Palate - Evaluating Taste Consensus Among Beer Reviewers\n",
    "\n",
    "---\n",
    "\n",
    "Group [**BlackAda**](https://en.wikipedia.org/wiki/Blackadder)\n",
    "\n",
    "> - Ludek Cizinsky ([ludek.cizinsky@epfl.ch](ludek.cizinsky@epfl.ch))\n",
    "> - Peter Nutter ([peter.nutter@epfl.ch](peter@nutter@epfl.ch))\n",
    "> - Pierre Lardet ([pierre.lardet@epfl.ch](pierre@lardet@epfl.ch))\n",
    "> - Christopher Bastin ([christopher.bastin@epfl.ch](christian@bastin@epfl.ch))\n",
    "> - Mika Senghaas ([mika.senghaas@epfl.ch](mika@senghaas@epfl.ch))\n",
    "\n",
    "ðŸ“£ Message for the TA: The initial section of this notebook is straightforward to execute and doesn't need any special preparation. For the `Analysis` segment, please adhere to the instructions in the [REPRODUCE](./REPRODUCE.md) document and ensure all required data is downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "---\n",
    "\n",
    "Navigating the world of beer reviews can be a daunting task for non-experts.\n",
    "Beer aficionados often describe brews as having nuanced flavors such as \"grassy\n",
    "notes\" and \"biscuity/ crackery malt,\" with hints of \"hay.\" But do these\n",
    "descriptions reflect the actual tasting experience? Following a\n",
    "\"wisdom-of-the-crowd\" approach, a descriptor can be considered meaningful if\n",
    "many, independent reviewers use similar descriptors for a beer's taste. To\n",
    "quantify consensus, we use natural language processing techniques to extract\n",
    "descriptors of a beer's taste and numerically represent these descriptors to\n",
    "compute similarity or consensus scores. The consensus scores between beer\n",
    "reviews will unveil whether there is a shared understanding of taste among beer\n",
    "geeks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "---\n",
    "\n",
    "We load the dependencies required for this project to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable continuous module reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "\n",
    "# External library\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokens import DocBin \n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Custom modules\n",
    "from src import utils\n",
    "from src import extractors\n",
    "from src import embedders\n",
    "from src.consensus import ConsensusBase, CosineSimilarity, Correlation, KullbackLeiblerDivergence, JensenShannonDivergence\n",
    "from src import visualise\n",
    "from src.aggregator import EmbeddingAggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And set some global variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorstyle = \"RdBu\"\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_palette(colorstyle)\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_colwidth = 150\n",
    "\n",
    "# Load SpaCy model\n",
    "NLP = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "DOC_BIN = DocBin(attrs=[\"LEMMA\", \"POS\"])\n",
    "LOAD_DOCS = True\n",
    "\n",
    "# URL for the full dataset\n",
    "DATA_URL = \"https://drive.google.com/u/2/uc?id=1IqcAJtYrDB1j40rBY5M-PGp6KNX-E3xq&export=download\"\n",
    "\n",
    "# Subsetting options\n",
    "SUBSET_DEMO = True # To run the demo on the full dataset follow the REPRODUCE.md instructions\n",
    "SUBSET_ANALYSIS = False \n",
    "NUM_SUBSET_DEMO = 10000 # Expects that `process_data.py --limit NUM_SUBSET_SAMPLES`\n",
    "\n",
    "# Paths\n",
    "ROOT_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n",
    "PROCESSED_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "\n",
    "# Random seed\n",
    "SEED = 42\n",
    "SHUFFLE_REVIEWS = False # Shuffle the embedding order\n",
    "\n",
    "# Type of metric\n",
    "METRIC = \"cosine\"  # cosine, kl, js, correlation\n",
    "\n",
    "# If this gives you error, then clone the website repo in the same directory as this repo\n",
    "IMAGES_PATH = os.path.join(\"..\", \"ada-2023-project-blackada-webpage\", \"src\", \"assets\")\n",
    "\n",
    "# Ensure data directory exists\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup of seaborn for our datastory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_path = 'font/Poppins-Regular.ttf' \n",
    "font_manager.fontManager.addfont(font_path)\n",
    "prop = font_manager.FontProperties(fname=font_path)\n",
    "\n",
    "# Light\n",
    "light_config = {\n",
    "    \"palette\": sns.color_palette([\n",
    "        \"#2654E8\", # Primary blue\n",
    "    ]),\n",
    "    \"style\": \"white\",\n",
    "    \"font\": prop.get_name(),\n",
    "    \"rc\": {\n",
    "        'axes.facecolor': (0,0,0,0), \n",
    "        'figure.facecolor':(0,0,0,0), \n",
    "        'axes.spines.left': False, \n",
    "        'axes.spines.bottom': False, \n",
    "        'axes.spines.right': False, \n",
    "        'axes.spines.top': False,\n",
    "        'axes.titleweight': 'bold',\n",
    "        'axes.labelpad': 10,  \n",
    "        'axes.titlepad': 10,  \n",
    "    },\n",
    "}\n",
    "\n",
    "dark_config = {\n",
    "    \"palette\": sns.color_palette([\n",
    "        \"#2654E8\", # Primary blue\n",
    "    ]),\n",
    "    \"style\": \"dark\",\n",
    "    \"rc\": {\n",
    "        'axes.facecolor': (0, 0, 0, 0),\n",
    "        'figure.facecolor': (0, 0, 0, 0),\n",
    "        'axes.spines.left': False,\n",
    "        'axes.spines.bottom': False,\n",
    "        'axes.spines.right': False,\n",
    "        'axes.spines.top': False,\n",
    "        'text.color': 'white',\n",
    "        'xtick.color': 'white',\n",
    "        'ytick.color': 'white',\n",
    "        'axes.labelcolor': 'white',\n",
    "        'axes.titleweight': 'bold',\n",
    "        'axes.labelpad': 10,  \n",
    "        'axes.titlepad': 10,  \n",
    "    },\n",
    "    \"font\": prop.get_name(),\n",
    "}\n",
    "\n",
    "\n",
    "SEABORN_CONFIGS = [(\"light\", light_config), (\"dark\", dark_config)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "---\n",
    "\n",
    "We will be working with the beer review data from the\n",
    "[BeerAdvocate](https://www.beeradvocate.com/) platform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Download\n",
    "\n",
    "Due to its size (uncompressed 1.6 GB), the dataset is not included in the\n",
    "repository but must be downloaded. The course staff has provided the data via\n",
    "Google Drive. On the first run of this notebook, we download the compressed data\n",
    "file from Google Drive and extract it to the `data` folder. The compressed file\n",
    "is ~1.5 GB in size.\n",
    "\n",
    "After extraction and the removal of unnecessary files (archives, ratings file,\n",
    "...), the data folder should contain the following files: `beers.csv`,\n",
    "`breweries.csv`, `users.csv`, `reviews.txt`. The total size of the data is ~2.9\n",
    "GB.\n",
    "\n",
    "_NB: Data loading takes around **~8min** on the first run. Subsequent runs of\n",
    "this cell are instant._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the BeerAdvocate dataset if it doesn't exist\n",
    "if not utils.raw_data_exists(DATA_DIR):\n",
    "    utils.download_data(DATA_URL, data_dir=DATA_DIR)\n",
    "print(f\"Raw beer review data downloaded to {DATA_DIR} âœ….\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "We would like to perform a series of pre-processing steps on the raw data to make it more amenable to analysis. These steps include:\n",
    "\n",
    "- Merge the reviews data with some additional meta-data about the beers, users\n",
    "  and breweries (e.g. beer style, user location, ...) and collect in a singe\n",
    "  multi-column DataFrame.\n",
    "- We cast each column to the correct type, e.g. `date` is converted to a\n",
    "  `datetime` object.\n",
    "- We remove any reviews with any missing values (as there are only very few\n",
    "  where this is the case)\n",
    "- We drop columns that are not relevant for our analysis\n",
    "- We rename columns and organise the DataFrame to have a multi-index with the\n",
    "  reviews for easier access\n",
    "- We compute `spacy` objects for each review which are crucial for extracting the\n",
    "  taste descriptors from the reviews. This is a computationally expensive step\n",
    "  and we therefore only compute the `lemma` and `pos` attributes of each token\n",
    "  which are the only ones we need for our analysis.\n",
    "- We add a new column `style` based on the `substyles` in the original data frame to have another way of grouping the beers\n",
    "\n",
    "_NB: Procesing all `2.5M` reviews into `spacy` objects takes a long time (~2h). Furthermore, the data is very large due to the metadata that spacy stores for each token. We therefore only load a subset of the metadata as specified in the `DocBin` object which we will need for our later analysis - namely the `lemma` and `pos` attributes. This significantly reduces the disk and memory usage._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data if it hasn't been processed yet\n",
    "if not utils.processed_data_exists(PROCESSED_DIR, NUM_SUBSET_DEMO if SUBSET_DEMO else None):\n",
    "    utils.process_data(DATA_DIR, PROCESSED_DIR, NLP, DOC_BIN, NUM_SUBSET_DEMO if SUBSET_DEMO else None)\n",
    "\n",
    "print(f\"Processed beer review data saved to {PROCESSED_DIR} âœ….\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "Next, we load the data into a Pandas DataFrame. On the first run, we load all\n",
    "the reviews from the `reviews.txt` file and populate it with some additional\n",
    "meta-data from the other files. We then save the DataFrame to a `.feather` file\n",
    "for faster loading in the future. On subsequent runs, we load the DataFrame from\n",
    "the `.feather` file if it exists.\n",
    "\n",
    "_NB: Running this cell for the first time reads in all `2.5M` reviews which\n",
    "takes **~7min**. Subsequent runs should be much faster, taking about **~1min**._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all reviews and a subset of reviews (10,000)\n",
    "reviews = utils.load_data(\n",
    "    PROCESSED_DIR, NLP, LOAD_DOCS, NUM_SUBSET_DEMO if SUBSET_DEMO else None\n",
    ")\n",
    "if SHUFFLE_REVIEWS:\n",
    "    random_indices = np.random.permutation(len(reviews))\n",
    "    reviews[(\"review\", \"text\")] = reviews[(\"review\", \"text\")].values[random_indices]\n",
    "    reviews[(\"review\", \"doc\")] = reviews[(\"review\", \"doc\")].values[random_indices]\n",
    "\n",
    "msg = \"Subset of Data\" if SUBSET_DEMO else \"Full Data\"\n",
    "print(f\"Loaded {len(reviews)} reviews âœ…. ({msg})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks\n",
    "\n",
    "During the data loading (`utils.load_data`) we perform some basic data\n",
    "pre-processing and merging. Specifically, we do the following:\n",
    "\n",
    "\n",
    "We check that each of these steps is performed correctly and that the data is\n",
    "consistent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that additional information is loaded in the reviews\n",
    "additional_cols = [(\"user\", \"location\")]\n",
    "\n",
    "for col in additional_cols:\n",
    "    err_msg = f\"âŒ Additional column {col} not loaded.\"\n",
    "    assert col in reviews.columns, err_msg\n",
    "print(f\"âœ… Additional columns loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that columns have correct type (e.g. review time is a datetime)\n",
    "example_types = {\n",
    "    (\"review\", \"date\"): \"datetime64[ns]\",\n",
    "    (\"review\", \"rating\"): \"float64\",\n",
    "    (\"review\", \"text\"): \"object\",\n",
    "}\n",
    "\n",
    "for col, dtype in example_types.items():\n",
    "    err_msg = f\"âŒ Column has type {reviews[col].dtype} but should be {dtype}\"\n",
    "    assert reviews[col].dtype == dtype, err_msg\n",
    "print(f\"âœ… All columns have correct type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that there are no missing values (NaNs)\n",
    "missing_values = reviews.isna().sum()\n",
    "\n",
    "err_msg = f\"âŒ There are {missing_values.sum()} missing values in the dataset!\"\n",
    "assert missing_values.sum() == 0, err_msg\n",
    "print(f\"âœ… There are no missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that index is integer\n",
    "indices = list(reviews.index) \n",
    "\n",
    "err_msg = f\"âŒ Index is not integer.\"\n",
    "assert list(range(len(indices))) == indices, err_msg\n",
    "print(f\"âœ… Indices are integers from 0 to {len(indices)-1}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Data\n",
    "\n",
    "Let's explore the data a bit. In this section we will investigate the total\n",
    "number of reviews and in various sub-groups, as well as understand basic\n",
    "statistics about the textual reviews.\n",
    "\n",
    "_Note: We have a full notebook with more detailed EDA of the data in the\n",
    "[`playground/eda.ipynb`](https://github.com/epfl-ada/ada-2023-project-blackada/blob/main/playground/eda.ipynb)\n",
    "notebook. In this notebook we focus on the parts of the data exploration that\n",
    "are important for our project._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 5 rows of the data\n",
    "reviews.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all data is in a single data frame with multi-column indexing. Each\n",
    "row corresponds to a single review of a beer and denotes the user (`user`), beer\n",
    "(`beer`) and brewery (`brewery`) meta information, as well as the actual review\n",
    "data (`review`) in separate column indices. For example, we can look at the keys\n",
    "individually for the first three reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-information on beer for first 3 samples\n",
    "reviews[\"beer\"].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-information on user for first 3 samples\n",
    "reviews[\"user\"].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-information on brewery for first 3 samples\n",
    "reviews[\"brewery\"].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about review for first 3 samples\n",
    "reviews[\"review\"].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, for each review, we have information on the following features:\n",
    "\n",
    "1. **Review** (`review`): Review Text, Ratings (Appearance, Aroma, Palate,\n",
    "   Taste, Overall, Rating), Date\n",
    "2. **User** (`user`): User ID, User Name, #Ratings, #Reviews, Joined Date,\n",
    "   Location\n",
    "3. **Beer** (`beer`): Beer ID, Beer Name, Beer Style, Beer Substyle, ABV (Alcohol By Volume),\n",
    "   #Ratings, #Reviews\n",
    "4. **Brewery** (`brewery`): Brewery ID, Brewery Name, Location, #Beers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groups\n",
    "\n",
    "In our analysis we want to compute the consensus between the language used in\n",
    "reviews of a) all beers, b) beers of the same style, c) beers from the same\n",
    "brewery and, finally, d) invidual beers. The hypothesis is that the\n",
    "finer-grained the grouping, the higher the consensus between the reviewers.\n",
    "However, for the analysis to be meaningful we need to ensure that there are\n",
    "enough reviews in each group. We therefore compute the number of reviews in each\n",
    "group and plot the distribution of the number of reviews per group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_beer_styles = reviews.beer[\"style\"].drop_duplicates()\n",
    "unique_beer_substyles = reviews.beer[\"substyle\"].drop_duplicates()\n",
    "unique_breweries = reviews.brewery.drop_duplicates()\n",
    "unique_beers = reviews.beer.drop_duplicates()\n",
    "\n",
    "print(f\"Number of unique beer styles: {len(unique_beer_styles)}\")\n",
    "print(f\"Number of unique beer substyles: {len(unique_beer_substyles)}\")\n",
    "print(f\"Number of unique breweries: {len(unique_breweries)}\")\n",
    "print(f\"Number of unique beers: {len(unique_beers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the sorted number of reviews per beer style, brewery and beer below. The\n",
    "y-axis is log-scaled to better show the distribution and we add a horizontal\n",
    "line to show the minimum number of reviews to `100` per beer style, brewery and\n",
    "beer that we require for our further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum number of reviews for a beer style, brewery, or beer to be included in the analysis\n",
    "MIN_REVIEWS = 5\n",
    "\n",
    "# Compute the number of reviews for each element in each group\n",
    "reviews_per_beer_style = (\n",
    "    reviews.groupby(by=(\"beer\", \"style\")).size().sort_values(ascending=False)\n",
    ")\n",
    "reviews_per_beer_substyle = (\n",
    "    reviews.groupby(by=(\"beer\", \"substyle\")).size().sort_values(ascending=False)\n",
    ")\n",
    "reviews_per_brewery = (\n",
    "    reviews.groupby(by=(\"brewery\", \"id\")).size().sort_values(ascending=False)\n",
    ")\n",
    "reviews_per_beer = (\n",
    "    reviews.groupby(by=(\"beer\", \"id\")).size().sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "# Plot number of reviews per beer style\n",
    "fig, axs = plt.subplots(ncols=4, figsize=(20, 5))\n",
    "for ax, reviews_per_group in zip(\n",
    "    axs, [reviews_per_beer_style, reviews_per_beer_substyle, reviews_per_brewery, reviews_per_beer]\n",
    "):\n",
    "    sns.lineplot(x=range(len(reviews_per_group)), y=reviews_per_group.values, ax=ax)\n",
    "    ax.plot(\n",
    "        [0, len(reviews_per_group)],\n",
    "        [MIN_REVIEWS, MIN_REVIEWS],\n",
    "        linestyle=\"--\",\n",
    "        color=\"black\",\n",
    "    )\n",
    "    a, b = reviews_per_group.index.names[0]\n",
    "    ax.set(\n",
    "        title=f\"#Reviews per {a.capitalize()} {b.capitalize()}\",\n",
    "        xlabel=\"Rank\",\n",
    "        ylabel=\"Counts (Log)\",\n",
    "        yscale=\"log\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out beer styles with less than MIN_REVIEWS reviews\n",
    "included_beer_styles = reviews_per_beer_style[\n",
    "    reviews_per_beer_style >= MIN_REVIEWS\n",
    "].index\n",
    "included_breweries = reviews_per_brewery[reviews_per_brewery >= MIN_REVIEWS].index\n",
    "included_beers = reviews_per_beer[reviews_per_beer >= MIN_REVIEWS].index\n",
    "\n",
    "# Create masks for filtering out beer styles with less than MIN_REVIEWS reviewsk\n",
    "min_reviews_beer_style_mask = reviews.beer[\"style\"].isin(included_beer_styles)\n",
    "min_reviews_breweries_mask = reviews.brewery[\"id\"].isin(included_breweries)\n",
    "min_reviews_beer_mask = reviews.beer[\"id\"].isin(included_beers)\n",
    "\n",
    "# Filter out reviews for beer styles with less than MIN_REVIEWS reviews\n",
    "original_reviews = reviews.copy()\n",
    "reviews = reviews[\n",
    "    min_reviews_beer_style_mask & min_reviews_breweries_mask & min_reviews_beer_mask\n",
    "].reset_index()\n",
    "\n",
    "print(\n",
    "    f\"âœ… Filtering done. Reviews after filtering: {len(reviews)} (Removed {len(original_reviews) - len(reviews)} reviews)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviews Statistics\n",
    "\n",
    "The textual reviews are central to our analysis and we will be using them to\n",
    "extract the taste descriptors. Let's look at some statistics about the reviews\n",
    "to ensure that they are of good quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's show some example reviews\n",
    "pd.DataFrame(reviews.review.head(10)[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this random sample of 10 reviews consists only of reviews that are\n",
    "very detailed and descriptive about the beer and its taste. This suggests that\n",
    "the majority of reviews are of good quality and suited for our analysis.\n",
    "However, we suspect that there might be some meaningless \"spam\" reviews that may\n",
    "skew our results. We will investigate this by checking for outliers in the\n",
    "review length. We use simple proxies for review length, namely the number of\n",
    "words and characters in the review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute character and word lengths of reviews\n",
    "character_lengths = reviews.review.text.str.len()\n",
    "word_lengths = reviews.review.text.apply(lambda x: len(x.split()))\n",
    "\n",
    "# Distribution of the number of ratings/ reviews per user\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(20, 5))\n",
    "sns.histplot(x=character_lengths, kde=True, ax=ax[0])\n",
    "sns.histplot(x=word_lengths, kde=True, ax=ax[1])\n",
    "\n",
    "character_lengths_stats = character_lengths.describe()\n",
    "word_lengths_stats = word_lengths.describe()\n",
    "\n",
    "ax[0].set(\n",
    "    title=\"Distribution of Character Lengths in Reviews\",\n",
    "    xlabel=\"Character Length\",\n",
    "    ylabel=\"Frequency\",\n",
    ")\n",
    "ax[1].set(\n",
    "    title=\"Distribution of Word Lengths in Reviews\",\n",
    "    xlabel=\"Word Lengths\",\n",
    "    ylabel=\"Frequency\",\n",
    ")\n",
    "\n",
    "# Show summary statistics\n",
    "pd.DataFrame(\n",
    "    [character_lengths_stats, word_lengths_stats],\n",
    "    index=[\"Character Lengths\", \"Word Lengths\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most reviews are around **~408 characters** and **~123 words** long.\n",
    "There is a slight right-skew in the distribution, meaning that there are some\n",
    "very long reviews. The very short reviews are probably not very helpful for our\n",
    "analysis as the numeric representation will not be meaningful. Let's look at\n",
    "those reviews to see if further processing is required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the shortest 0.1% of reviews (by character count)\n",
    "n = int(len(reviews) * 0.001)\n",
    "character_sorted = list(character_lengths.sort_values().index.values)\n",
    "shortest_character_length_reviews = reviews.review[\n",
    "    reviews.index.isin(character_sorted[:n])\n",
    "]\n",
    "\n",
    "pd.DataFrame(shortest_character_length_reviews.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the shortest 0.1% of reviews (by word count)\n",
    "n = int(len(reviews) * 0.001)\n",
    "words_sorted = list(word_lengths.sort_values().index.values)\n",
    "shortest_word_length_reviews = reviews.review[reviews.index.isin(\n",
    "    words_sorted[:n])]\n",
    "\n",
    "pd.DataFrame(shortest_word_length_reviews.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspecting the shortest reviews, we can see that most of the shortest\n",
    "reviews by character count are actually regular reviews that are just short.\n",
    "However, in the reviews with very little words we can see some \"spam\" reviews\n",
    "that are not very helpful for our analysis. It is likely that our extractors are\n",
    "going to struggle with these kinds of reviews. Therefore, we remove all reviews\n",
    "that have less than `10` words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the shortest reviews by word count from the dataset\n",
    "MIN_WORDS = 10\n",
    "not_filtered_review = reviews.copy()\n",
    "reviews = reviews[word_lengths >= MIN_WORDS]\n",
    "\n",
    "print(\n",
    "    f\"Removed {(word_lengths < MIN_WORDS).sum()} reviews with less than {MIN_WORDS} words âœ…\"\n",
    ")\n",
    "print(f\"Number of reviews: {len(reviews)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractors\n",
    "\n",
    "Before we embed reviews into a numerical representation, we preprocess them\n",
    "using different **extractors modules**. For this project, we have considered the\n",
    "following method:\n",
    "\n",
    "(1) `DummyExtractor`: This is a dummy extractor that does not do any\n",
    "preprocessing. It simply returns the input text as is.\n",
    "\n",
    "(2) `LemmaExtractor`: Tokenizes the text and then uses only _lemmas_ of the\n",
    "extracted tokens. A lemma is the base form of a word. For example, the lemma of\n",
    "**was** is **be**. Thus, the `LemmaExtractor` might be thougt of as a text\n",
    "normaliser which maps all tokens to the normalised space.\n",
    "\n",
    "(3) `AdjectiveExtractor`: As the name suggests, extract tokens which were\n",
    "classified by `spaCy` as **adjectives**.\n",
    "\n",
    "(4) `StopWordExtractor`: Removes common words with no semantic value, this implemantaion also uses lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all extractor models\n",
    "extractor_models: list[extractors.ExtractorBase] = [\n",
    "    extractors.DummyExtractor(),\n",
    "    extractors.LemmaExtractor(),\n",
    "    extractors.AdjectiveExtractor(),\n",
    "    extractors.StopwordExtractor()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to understand the behaviour of each of the extractors in detail. To do\n",
    "this, we process an example review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define demo review\n",
    "demo_review = \"\"\"Pours with a frothy head then settles to a thin head with thin lacing. \n",
    "Transparent. Golden to bronze in color. Dry grains. \n",
    "Light notes of citrus - orange. Pilsner-esque. Very light malt sweetness - caramel. \n",
    "Moves to a dry hoppy-ness. Light bodied. Dry. Somewhat chalky. Meh. \n",
    "Just average. Not one I would suggest to a friend, but thank for the organic \n",
    "ingredients.\n",
    "\"\"\"\n",
    "\n",
    "# Preprocess the example with Spacy\n",
    "processed_demo_review = [NLP(demo_review)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the extractors against the example\n",
    "transformed_all = []\n",
    "for extractor in extractor_models:\n",
    "    transformed_example = extractor.transform(processed_demo_review)\n",
    "    transformed_all.append(transformed_example[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the `DummyExtractor`, we can use it as a reference baseline for\n",
    "the other two extractors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DummyExtractor:\\n\", transformed_all[0].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `LemmaExtractor` next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LemmaExtractor\\n\", transformed_all[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the text below shows, `LemmaExtractor` has normalised the words to their base\n",
    "form (lemma), a couple of examples:\n",
    "\n",
    "(1) `grains` -> `grain` (get rid of the plural form)\n",
    "\n",
    "(2) `settles` -> `settle` (remove `s` from the he/she/it form)\n",
    "\n",
    "(3) `bodied` -> `body` (stem form)\n",
    "\n",
    "Apart from the lemmatisation, we can also see that how `spaCy` tokenizes the\n",
    "text. In particular, it treats punctuation marks as separate tokens. For\n",
    "example, `.` is a separate token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we run the `AdjectiveExtractor` on the example review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AdjectiveExtractor\\n\", transformed_all[2].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, looking at the `AdjectiveExtractor`, we can see that it strips the text\n",
    "to only adjectives, thereby potentially losing some useful information. On the\n",
    "other hand, for the purposes of our analysis, this might be in fact useful as we\n",
    "only want our embeddings be based on the descriptive words related to beer and\n",
    "avoid the noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finnaly, we we run `StopWordExtractor` with lemmatisation on the example review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"StopwordExtractor\\n\", transformed_all[3].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The StopwordExtractor eliminates common stopwords and lemmatizes the remaining text. This approach is chosen for our analysis as it filters out irrelevant words while retaining meaningful non-adjective terms, like **caramel**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will run the extractors against the reviews and show the word frequency\n",
    "and the 10 most frequent words for each extractor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We map the list of docs to the list of preprocessed strings\n",
    "extracted_reviews = [\n",
    "    extractor.transform(reviews.review.doc) for extractor in extractor_models\n",
    "]\n",
    "frequencies = [utils.get_word_frequency(text) for text in extracted_reviews]\n",
    "\n",
    "# Plot the word frequency of top-10 words for each extractor\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 5))\n",
    "for ax, freq, extractor in zip(axes, frequencies, extractor_models):\n",
    "    sns.barplot(x=\"frequency\", y=\"word\", data=freq.head(10), ax=ax)\n",
    "    ax.set_title(extractor.name)\n",
    "    ax.set_xlabel(\"Word Frequency\")\n",
    "    ax.set_ylabel(\"Word\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, each of the extractors works as expected. Given the manual\n",
    "inspection of the extraction process, we hypothesise that the\n",
    "`StopWordRemover` with lemmatization is the most suitable one for our task because the extracs both adjectives and other nouns that are relevant for a description of a beer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedders\n",
    "\n",
    "We need an embedding module to turn the extracted information from the reviews\n",
    "into a numeric representation. Let's go over how each one works.\n",
    "\n",
    "- CountEmbeddors uses sklearn's\n",
    "  [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "  Count vectorisation simply assigns each word in the vocabulary to a variable\n",
    "  in the feature vector, and the values are the counts of each word.\n",
    "\n",
    "- TFIDF is similar to CountVectorizer, but also multiplies by an 'inverse\n",
    "  document frequency' term. This weights a word in the vocabular by how\n",
    "  frequently it appears in the corpus. Very common words are penalised, and\n",
    "  rarer words are given more weight. This also uses sklearn's\n",
    "  [TFIDFVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "  This is done using the following formula:\n",
    "\n",
    "$$w_{i,j} = tf_{i,j} \\times \\frac{N}{df_i}$$\n",
    "\n",
    "where $w_{i,j}$ denotes the TFIDF of the $i$ th term in review $j$, $tf_{i,j}$\n",
    "is the 'term frequency' (the count vectorization) of term $i$ in review $j$, $N$\n",
    "is the total number of reviews and $df_i$ is the 'document frequency' of term\n",
    "$i$ i.e. the number of documents in which $i$ appears. This second half of the\n",
    "equation corresponds to the 'inverse document frequency' (IDF) of TFIDF.\n",
    "\n",
    "- BERTEmbeddor uses `bert-base-uncased`\n",
    "  [from HuggingFace](https://huggingface.co/bert-base-uncased). BERT is a\n",
    "  bidirectional encoder-only transformer. There are many options for extracting\n",
    "  embeddings from the model since there are 12 layers, and an embbedding for\n",
    "  each token input. Currently, the implementation takes the penultimate hidden\n",
    "  state of the model and takes the mean across all tokens in the input (see\n",
    "  [this guide](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/)).\n",
    "\n",
    "- SentenceTransformerEmbeddor uses the recommended `all-MiniLM-L6-v2` model from\n",
    "  the `sentence-transformers`\n",
    "  [library](<(https://www.sbert.net/docs/pretrained_models.html)>). These models\n",
    "  take outputs from BERT, conduct pooling similar to above (e.g. by default,\n",
    "  mean of last layer), and are trained on various sentence-related NLP problems\n",
    "  using\n",
    "  [Siamese networks](https://towardsdatascience.com/a-friendly-introduction-to-siamese-networks-85ab17522942).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the embedder models and run them first on some dummy reviews\n",
    "and then a small sample of the actual reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalise embedders\n",
    "embedding_models: list[embedders.EmbedderBase] = [\n",
    "    embedders.CountEmbedder(),\n",
    "    embedders.TfidfEmbedder(sparse_output=False),\n",
    "    embedders.BertEmbedder(),\n",
    "    embedders.SentenceTransformerEmbedder(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we compute the cosine similary between the first sentence and each\n",
    "subsequent sentence for each of the embedding models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with some sample reviews\n",
    "embedder_demo = pd.DataFrame(\n",
    "    {\n",
    "        \"text\": [\n",
    "            \"The beer is nice, with sweet nutty flavours\",\n",
    "            \"This is a very different sentence\",\n",
    "            \"Not sweet enough. I like my beer sweet. \",\n",
    "            \"Not sweet at all. Terrible beer. \",\n",
    "            \"Not sweet at all. But I like bitter beers so it is a nice beer. \",\n",
    "            \"Piss yellow beer\",\n",
    "            \"Sweet beer\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialise results dataframe\n",
    "results = pd.DataFrame(\n",
    "    index=[model.name for model in embedding_models],\n",
    "    columns=[f\"Similarity {i}\" for i in range(1, len(embedder_demo))],\n",
    ")\n",
    "\n",
    "# Compute the similarity between the first and the nth sentence\n",
    "for i in range(1, len(embedder_demo)):\n",
    "    for model in embedding_models:\n",
    "        similarity = utils.compute_similarity(\n",
    "            model, embedder_demo[\"text\"][0], embedder_demo[\"text\"][i]\n",
    "        )\n",
    "        results.loc[model.name, f\"Similarity {i}\"] = similarity\n",
    "\n",
    "print(\"Similarity between first and nth sentence:\")\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the obvious pitfall of using CountVectorizer and TFIDF - they\n",
    "lose all context. Sentences 3 and 4 would ideally have the lowest similairty\n",
    "with sentence 1 since they are opposite in meaning. However, these samples all\n",
    "use the same words which Count and TFIDF interpret as therefore being similar.\n",
    "If, during the pipeline, we were to group beers by some measure that affects\n",
    "their sweetness, then in order to confirm out hypothesis we would like to see an\n",
    "increase of similarity inside each group, but we may lower similarity due to\n",
    "negations.\n",
    "\n",
    "However, BERT and SentenceTransformers are not necessarily better. The values\n",
    "are far less interpretable, with sentence embeddor falling for a similar\n",
    "negation trap since similarities 3 and 4 are higher than 7. Interestingly,\n",
    "SentenceTransformer was far better than BERT at differentiating between\n",
    "sentences on different topic matters (similarity 2). BERT's scores are all\n",
    "broadly similar, and roughly gets the order in line what we might expect, but we\n",
    "have little faith that this translates any better than sentence transformer to\n",
    "the real reviews since these little samples play into BERT's context-aware\n",
    "strengths.\n",
    "\n",
    "However, for now, we will try to make conclusions using tf-idf. It is the most\n",
    "interpretable (we can get out the most impactful words at the end), and so long\n",
    "as there are enough reviews that are long enough, we should see a meaningful\n",
    "vocabulary emerge. If the tfidf embeddings seem to be limiting us in the future,\n",
    "we can experiment with other methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try with some real sample reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with some sample reviews\n",
    "embedder_sample = pd.DataFrame(\n",
    "    {\"text\": reviews.sample(4, random_state=0).review.text.values.tolist()}\n",
    ")\n",
    "\n",
    "# Initialise results dataframe\n",
    "results = pd.DataFrame(\n",
    "    index=[model.name for model in embedding_models],\n",
    "    columns=[f\"Similarity {i}\" for i in range(1, len(embedder_sample))],\n",
    ")\n",
    "\n",
    "# Compute the similarity between the first and the nth sentence\n",
    "for i in range(1, len(embedder_sample)):\n",
    "    for model in embedding_models:\n",
    "        similarity = utils.compute_similarity(\n",
    "            model, embedder_sample[\"text\"][0], embedder_sample[\"text\"][i]\n",
    "        )\n",
    "        results.loc[model.name, f\"Similarity {i}\"] = similarity\n",
    "\n",
    "print(\"Similarity between first and nth sentence:\")\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisinlgy, Count and TFIDF agree on ordering of similarity. However, BERT\n",
    "and SentenceTransformer disagree both with this ordering and each other. BERT is\n",
    "the less 'sure', with very high and close values, as in the previous example.\n",
    "\n",
    "Reading the reviews, it's very hard to define what the ordering _should_ be,\n",
    "therefore it is hard to define which embedder has done a better job in this\n",
    "sample. Further investigation will be carried out for P3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the large size of the dataset, we will now load precomputed embeddings of all of the reviews. These embeddings were computed using the `StopWordExtractor` and `TFIDFEmbeddor`. How to get this data is explained in the [Reporduce](./REPRODUCE.md) file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all reviews and emebddings\n",
    "reviews_all = utils.load_data(PROCESSED_DIR, None, False, None)\n",
    "embeddings_all = utils.load_embeddings(PROCESSED_DIR, None)  # sparse matrix\n",
    "vocab = utils.load_vocab(PROCESSED_DIR)\n",
    "assert len(reviews_all) == embeddings_all.shape[0]\n",
    "print(f\"âœ… Loaded {len(reviews_all)} reviews and embeddings.\")\n",
    "\n",
    "\n",
    "# Filter out beer styles with less than MIN_REVIEWS reviews\n",
    "MIN_REVIEWS = 50\n",
    "MAX_REVIEWS = None\n",
    "MIN_WORDS = 50\n",
    "embeddings_all, reviews_all = utils.filter_data(\n",
    "    embeddings_all, reviews_all, MIN_REVIEWS, MAX_REVIEWS, MIN_WORDS\n",
    ")  # Add min words filter\n",
    "assert len(reviews_all) == embeddings_all.shape[0]\n",
    "print(f\"âœ… Filtered {len(reviews_all)} reviews and embeddings.\")\n",
    "\n",
    "# Shuffle the embeddings to prove that the effect isn't random\n",
    "if SHUFFLE_REVIEWS:\n",
    "    random_indices = np.random.permutation(embeddings_all.shape[0])\n",
    "    embeddings_all = embeddings_all[random_indices]\n",
    "    print(f\"âœ… Shuffled embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding visualisation\n",
    "Here we chose manually three disinct beer styles that we will use to visualise the embeddings. From each of these styles we picked one of its most popular representatives from at each level of aggregation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand picked styles\n",
    "styles = [\"Specialty Beers\", \"Stouts and Porters\", \"Pale Lagers and Pilsners\"]\n",
    "\n",
    "# Representative substyles for each style\n",
    "substyle = [\n",
    "    \"Fruit / Vegetable Beer\",\n",
    "    \"American Double / Imperial Stout\",\n",
    "    \"German Pilsener\",\n",
    "]\n",
    "\n",
    "# Representative beers for each substyle\n",
    "beers = [\n",
    "    \"Samuel Adams Cherry Wheat\",\n",
    "    \"Founders KBS (Kentucky Breakfast Stout)\",\n",
    "    \"Prima Pils\",\n",
    "]\n",
    "groups = [styles, substyle, beers]\n",
    "aggregators = [(\"beer\", \"style\"), (\"beer\", \"substyle\"), (\"beer\", \"name\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand the embeddings a bit better by plotting them in 2D using SVD. Other methods such as t-SNE are too computationally expensive to run on the full dataset but produce nicer plots. `TruncatedSVD` is very fast on sparse matrices, but it only distinguishes clusters along the directions of the highest singular values or their combinations. Therefore, it cannot distinguish many styles or subtle variations within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed into 2 dimensions\n",
    "svd = TruncatedSVD(n_components=2, n_iter=10, random_state=42)\n",
    "reduced_embeddings = svd.fit_transform(embeddings_all)\n",
    "reduced_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup consistent colors for each group\n",
    "n = len(groups)\n",
    "colors = sns.color_palette(\"husl\", n_colors=n)\n",
    "group_colors = [dict(zip(group, colors)) for group in groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=len(aggregators), figsize=(5 * len(aggregators), 5))\n",
    "fig.tight_layout(pad=0)\n",
    "\n",
    "for i, (ax, agg, group) in enumerate(zip(axs, aggregators, groups)):\n",
    "    subset = reviews_all[reviews_all[agg].isin(group)]\n",
    "\n",
    "    colors = group_colors[i]\n",
    "    visualise.embeddings(\n",
    "        title=f\"By {agg[1].capitalize()}\",\n",
    "        embeddings=reduced_embeddings,\n",
    "        hue=reviews_all[agg],\n",
    "        subset=subset.index,\n",
    "        plot_type=\"kde\",\n",
    "        fill=True,\n",
    "        alpha=0.5,\n",
    "        plot_legend=True,\n",
    "        ax=ax,\n",
    "        color_palette=colors,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the same thing just as a scetter plot ðŸ’”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=len(aggregators), figsize=(5 * len(aggregators), 5))\n",
    "fig.tight_layout(pad=0)\n",
    "for i, (ax, agg, group) in enumerate(zip(axs, aggregators, groups)):\n",
    "    # Filter out all reviews that are not in the group\n",
    "    subset = reviews_all[reviews_all[agg].isin(group)]\n",
    "\n",
    "    # Normalize the number of reviews per group to the smallest group\n",
    "    min_number_samples = subset.groupby(agg).size().min()\n",
    "    subset = subset.groupby(agg).sample(n=min_number_samples, random_state=SEED)\n",
    "\n",
    "    cmap = sns.cubehelix_palette(start=0, light=1, as_cmap=True)\n",
    "    visualise.embeddings(\n",
    "        title=f\"By {agg[1].capitalize()}\",\n",
    "        embeddings=reduced_embeddings,\n",
    "        hue=reviews_all[agg],\n",
    "        subset=subset.index,\n",
    "        plot_type=\"scatter\",\n",
    "        # fill=True,\n",
    "        alpha=0.2,\n",
    "        color_palette=group_colors[i],\n",
    "        plot_legend=False,\n",
    "        ax=ax,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consensus\n",
    "#### Overview\n",
    "In this section, we initialize the consensus models to quantify the agreement or similarity within groups of data. This is crucial for understanding the patterns and trends in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Consensus Metrics for TF-IDF Analysis\n",
    "\n",
    "#### Overview\n",
    "\n",
    "In our analysis, we apply various metrics to the rows of a TF-IDF matrix to measure the consensus or similarity within groups. A TF-IDF matrix represents the importance of words in documents within a corpus, making these metrics particularly relevant for text analysis.\n",
    "\n",
    "#### Description of Metrics\n",
    "\n",
    "1. **Cosine Similarity**: \n",
    "   - Ideal for comparing rows of a TF-IDF matrix, as it measures the cosine of the angle between two vectors. \n",
    "   - Effectively assesses similarity in terms of orientation in a multi-dimensional space, making it robust against differences in document length.\n",
    "   - Ranges from -1 (exactly opposite) to 1 (exactly the same), with 0 indicating independence.\n",
    "\n",
    "2. **Kullback-Leibler (KL) Divergence**: \n",
    "   - Measures the divergence of one probability distribution from another.\n",
    "   - In the context of TF-IDF, it can highlight how the word distribution of one document diverges from another.\n",
    "   - **Adjusted for Similarity**: We use $ \\frac{1}{1 + KL} $ to quantify similarity, with values closer to 1 indicating more similarity.\n",
    "\n",
    "3. **Jensen-Shannon (JS) Divergence**: \n",
    "   - A symmetric and smoothed variant of KL Divergence.\n",
    "   - **Adjusted for Similarity**: We apply $\\frac{1}{1 + JS} $, where values near 1 suggest high similarity.\n",
    "\n",
    "4. **Correlation (Pearson)**: \n",
    "   - Assesses the linear relationship between two sets of data.\n",
    "   - In TF-IDF analysis, it helps in understanding the linear association between the term frequencies of different documents.\n",
    "\n",
    "#### Implications and Performance\n",
    "\n",
    "- **Cosine Similarity** is highly effective in text analysis, particularly with TF-IDF where it accounts for the relative importance of terms in documents.\n",
    "- **KL and JS Divergence** provide a deeper insight into how the term distributions vary between documents.\n",
    "- **Pearson Correlation** offers an understanding of linear relationships in term frequencies.\n",
    "- Running metrics other than Cosine Similarity can be more computationally intensive but may yield other results than Cosine Similarity that was used for the main analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the consensus models\n",
    "consensus: ConsensusBase\n",
    "if METRIC == \"cosine\":\n",
    "    consensus = CosineSimilarity()\n",
    "elif METRIC == \"kl\":\n",
    "    consensus = KullbackLeiblerDivergence()\n",
    "elif METRIC == \"js\":\n",
    "    consensus = JensenShannonDivergence()\n",
    "elif METRIC == \"correlation\":\n",
    "    consensus = Correlation()\n",
    "else:\n",
    "    raise ValueError(f\"Unknown metric {METRIC}\")\n",
    "print(f\"âœ… Using {type(consensus).__name__} consensus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing Aggregators\n",
    "To facilitate the analysis of consensus within different beer categories, we initialize aggregator classes. These aggregators will help us group the data by different levels such as beer name, style, substyle, and overall general category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the aggregators\n",
    "beer_aggregator = EmbeddingAggregator(\n",
    "    embeddings_all, reviews_all, consensus, (\"beer\", \"name\")\n",
    ")\n",
    "style_aggregator = EmbeddingAggregator(\n",
    "    embeddings_all, reviews_all, consensus, (\"beer\", \"style\")\n",
    ")\n",
    "substyle_aggregator = EmbeddingAggregator(\n",
    "    embeddings_all, reviews_all, consensus, (\"beer\", \"substyle\")\n",
    ")\n",
    "gereral_aggregator = EmbeddingAggregator(embeddings_all, reviews_all, consensus, None)\n",
    "\n",
    "print(f\"â„¹ï¸ Beer groups: {len(beer_aggregator.groups)}\")\n",
    "print(f\"â„¹ï¸ Style groups: {len(style_aggregator.groups)}\")\n",
    "print(f\"â„¹ï¸ Substyle groups: {len(substyle_aggregator.groups)}\")\n",
    "print(f\"â„¹ï¸ General groups: {len(gereral_aggregator.groups)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform a detailed statistical analysis on the consensus distributions for each category. Please note, this process is time-intensive and is expected to take approximately `15 minutes`. The analysis proceeds as follows: For each hierarchical level of data aggregation â€“ encompassing the overall beer category, specific beer styles, substyles, and individual beers â€“ we select either the maximum number of samples defined by `MAX_SAMPLES` or the actual number of reviews available for that category. Next, we calculate the consensus matrix for these data points, utilizing the designated `METRIC`. Finally, we determine the average pairwise similarity for these groups and record the results in the consensus_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a nice df\n",
    "levels = [\"General\", \"Style\", \"Substyle\", \"Beer\"]\n",
    "consensus_levels = [\n",
    "    gereral_aggregator,\n",
    "    style_aggregator,\n",
    "    substyle_aggregator,\n",
    "    beer_aggregator,\n",
    "]\n",
    "MAX_SAMPLES = 10000\n",
    "consensus_df = []\n",
    "for level, consensus_level in zip(levels, consensus_levels):\n",
    "    for group in tqdm(consensus_level.groups, desc=f\"Processing Group {level}\"):\n",
    "        con = consensus_level.get_consensus_distribution(\n",
    "            group, max_samples=MAX_SAMPLES\n",
    "        ).mean()\n",
    "\n",
    "        consensus_df.append(\n",
    "            {\n",
    "                \"level\": level,\n",
    "                \"group\": group,\n",
    "                \"consensus\": con,\n",
    "            }\n",
    "        )\n",
    "        dis = consensus_level.get_embeddings_by_group(group).mean(axis=0)\n",
    "\n",
    "consensus_df = pd.DataFrame(consensus_df)\n",
    "print(f\"ï¸âœ… Created consensus dataframe of shape {consensus_df.shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we display the mean average of the individual means of each group. The Number of samples increases with the level of aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by levels list\n",
    "level_stats = consensus_df.groupby(by=[\"level\"]).describe().reindex(levels, level=0)\n",
    "level_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the consensus score does not change a lot, the mean consensus across\n",
    "all groups increases with increasing specificity of the grouping. This is promising\n",
    "for confirming our hypothesis that language used differs between beer types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of mean consensus scores\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(20, 6))\n",
    "fig.suptitle(\"Distribution Shift\", fontsize=16)\n",
    "\n",
    "# Boxplot of consensus scores per level\n",
    "sns.boxplot(\n",
    "    data=consensus_df[consensus_df[\"level\"] != \"General\"],\n",
    "    x=\"level\",\n",
    "    y=\"consensus\",\n",
    "    hue=\"level\",\n",
    "    order=levels[1:],\n",
    "    showfliers=False,\n",
    "    palette=\"CMRmap\",\n",
    "    ax=axs[0],\n",
    ")\n",
    "\n",
    "# Distplot of consensus scores per level\n",
    "sns.histplot(\n",
    "    data=consensus_df[consensus_df[\"level\"] != \"General\"],\n",
    "    x=\"consensus\",\n",
    "    hue=\"level\",\n",
    "    stat=\"probability\",\n",
    "    kde=True,\n",
    "    fill=True,\n",
    "    common_norm=False,\n",
    "    palette=\"CMRmap\",\n",
    "    ax=axs[1],\n",
    ")\n",
    "\n",
    "print(f\"âœ… Plotted distribution of all consensus scores in levels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Tests\n",
    "\n",
    "We pefrom a statistical test with the null hypothesis that the average consensus scores in each level of grouping are the same. We use the ANOVA test to test this hypothesis at the 1% significance level. We have different number of samples in each group, so we use the type 2 ANOVA test which is more robust to unequal sample sizes. [Post about differnet types of ANOVA](https://www.r-bloggers.com/2011/03/anova-%e2%80%93-type-iiiiii-ss-explained/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first need to fit an OLS model\n",
    "model = ols(\"consensus ~ C(level)\", data=consensus_df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "anova_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"The test statistic is {anova_table.iloc[0]['F']:.2f} and the p-value is {anova_table.iloc[0]['PR(>F)']:.2f}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the p-value is very small, so we reject the null hypothesis and conclude that the average consensus scores in each level of grouping are not the same. This is promising for confirming our hypothesis that language used differs between beer types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We double check that this insn't a result of the methodology means were computed by re-running the whole notebook with `SHUFFLE=True`. Now we should get a large p value and fail to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would also like to know which `Levels` are significantly different from each other. We use Tukey's HSD test to test this hypothesis at the 1% significance level. We see that only the **Beer** to **Substyle** and **Beer** to **Style** comparisons are significantly different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turkey_results = pairwise_tukeyhsd(\n",
    "    consensus_df[\"consensus\"], consensus_df[\"level\"], alpha=0.01\n",
    ")\n",
    "turkey_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layman's Guide to the Beer Lexicon\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode, seaborn_cfg in SEABORN_CONFIGS:\n",
    "\n",
    "    # Set the config for given mode light / dark\n",
    "    sns.set(**seaborn_cfg)\n",
    "\n",
    "    # Compute the average\n",
    "    M = embeddings_all.mean(axis=0)\n",
    "    average_embedding = np.squeeze(np.asarray(M))\n",
    "\n",
    "    # Select top K\n",
    "    top_k = 10\n",
    "    top_k_embeddings = np.argsort(average_embedding)[::-1][:top_k]\n",
    "    top_k_features = vocab[top_k_embeddings]\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(11, 5))\n",
    "    sns.barplot(y=top_k_features, x=average_embedding[top_k_embeddings], ax=ax, orient=\"h\")\n",
    "    ax.set_title(f\"Top {top_k} Features across all Reviews\")\n",
    "    ax.set_xlabel(\"Feature\")\n",
    "    ax.set_ylabel(\"Average Embedding\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    save_path = os.path.join(IMAGES_PATH, f\"{mode}_top_features.png\")\n",
    "    fig.savefig(save_path, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "std",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
